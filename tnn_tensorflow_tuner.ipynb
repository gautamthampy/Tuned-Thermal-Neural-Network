{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner shap tf-explain tqdm"
      ],
      "metadata": {
        "id": "2Jpo6Jhrh8L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# CELL 1: MAIN CODE - RUN THIS FIRST\n",
        "# ===============================================================\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "Comprehensive hyper-parameter tuner for Thermal-NN.\n",
        "\n",
        "• Optimized for Google Colab T4 GPU\n",
        "• Works on CPU, GPU (Colab T4/V100) or headless servers.\n",
        "• Handles any CSV that has a numeric target column named 'pm'.\n",
        "• Writes:\n",
        "      artifacts/tnn_best.keras\n",
        "      artifacts/metrics.json\n",
        "\n",
        "Usage in Google Colab:\n",
        "    Just run all cells in order, modify the CONFIG section as needed.\n",
        "\"\"\"\n",
        "\n",
        "# --------- 0. Configuration (modify these as needed) ----------------\n",
        "CONFIG = {\n",
        "    'csv_path': '/content/measures_v2.csv',        # Path to your CSV file\n",
        "    'trials': 20,                                   # Number of hyperparameter trials\n",
        "    'epochs': 30,                                   # Epochs per trial\n",
        "    'final_epochs': 50,                            # Epochs for final model\n",
        "    'val_split': 0.2,                              # Validation split ratio\n",
        "    'batch_size': 512,                             # Batch size (increased for better GPU usage)\n",
        "    'random_seed': 42                              # Random seed for reproducibility\n",
        "}\n",
        "\n",
        "# --------- 1. environment tweaks (before TF import) -----------------\n",
        "import os\n",
        "os.environ.setdefault(\"TF_FORCE_GPU_ALLOW_GROWTH\", \"true\")\n",
        "os.environ.setdefault(\"TF_GPU_ALLOCATOR\", \"cuda_malloc_async\")\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")  # Reduce TF logging\n",
        "\n",
        "# --------- 2. imports ------------------------------------------------\n",
        "from pathlib import Path\n",
        "import json, gc, time, sys\n",
        "import numpy as np, pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# Install keras-tuner if not available\n",
        "try:\n",
        "    import keras_tuner as kt\n",
        "except ImportError:\n",
        "    print(\"Installing keras-tuner...\")\n",
        "    import subprocess\n",
        "    subprocess.run([\"pip\", \"install\", \"keras-tuner\"], check=True)\n",
        "    import keras_tuner as kt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras import backend as K\n",
        "\n",
        "# --------- 3. GPU setup and verification ----------------------------\n",
        "def setup_gpu():\n",
        "    \"\"\"Configure GPU settings for optimal performance in Colab\"\"\"\n",
        "    print(\"Setting up GPU configuration...\")\n",
        "\n",
        "    # Check TensorFlow version\n",
        "    print(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "    # List available devices\n",
        "    physical_devices = tf.config.list_physical_devices()\n",
        "    print(f\"Available devices: {[device.name for device in physical_devices]}\")\n",
        "\n",
        "    # Configure GPU memory growth\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            # Enable memory growth for all GPUs\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            print(f\"✅ GPU memory growth enabled for {len(gpus)} GPU(s)\")\n",
        "\n",
        "            # Verify GPU is accessible\n",
        "            with tf.device('/GPU:0'):\n",
        "                test_tensor = tf.constant([[1.0]])\n",
        "                result = tf.matmul(test_tensor, test_tensor)\n",
        "                print(f\"✅ GPU test successful: {result.numpy()}\")\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            print(f\"⚠️  GPU setup error: {e}\")\n",
        "            print(\"Falling back to CPU\")\n",
        "\n",
        "    else:\n",
        "        print(\"⚠️  No GPU detected - using CPU\")\n",
        "\n",
        "    return len(gpus) > 0\n",
        "\n",
        "# Set reproducibility\n",
        "tf.keras.utils.set_random_seed(CONFIG['random_seed'])\n",
        "np.random.seed(CONFIG['random_seed'])\n",
        "\n",
        "# --------- 4. load & scale data -------------------------------------\n",
        "def load_scaled(csv_path, val_split=0.2):\n",
        "    \"\"\"Load and scale data with improved error handling\"\"\"\n",
        "    print(f\"Loading data from {csv_path}\")\n",
        "\n",
        "    # Handle both string and Path objects\n",
        "    csv_path = Path(csv_path)\n",
        "\n",
        "    if not csv_path.exists():\n",
        "        raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"Data shape: {df.shape}\")\n",
        "        print(f\"Columns: {list(df.columns)}\")\n",
        "\n",
        "        if \"pm\" not in df.columns:\n",
        "            available_cols = list(df.columns)\n",
        "            raise KeyError(f\"'pm' target column not found in CSV. Available columns: {available_cols}\")\n",
        "\n",
        "        # Check for missing values\n",
        "        missing_count = df.isnull().sum().sum()\n",
        "        if missing_count > 0:\n",
        "            print(f\"⚠️  Warning: Found {missing_count} missing values, filling with column means\")\n",
        "            df = df.fillna(df.mean())\n",
        "\n",
        "        X_raw = df.drop(columns=[\"pm\"]).values.astype(\"float32\")\n",
        "        y_raw = df[\"pm\"].values.reshape(-1, 1).astype(\"float32\")\n",
        "\n",
        "        print(f\"Features shape: {X_raw.shape}, Target shape: {y_raw.shape}\")\n",
        "\n",
        "        xs, ys = StandardScaler(), StandardScaler()\n",
        "        X = xs.fit_transform(X_raw)\n",
        "        y = ys.fit_transform(y_raw).ravel()\n",
        "\n",
        "        # Stratified split to maintain data distribution\n",
        "        idx = np.random.permutation(len(X))\n",
        "        split = int(len(X) * (1 - val_split))\n",
        "        x_train, y_train = X[idx[:split]], y[idx[:split]]\n",
        "        x_val, y_val = X[idx[split:]], y[idx[split:]]\n",
        "\n",
        "        print(f\"Train set: {x_train.shape}, Validation set: {x_val.shape}\")\n",
        "\n",
        "        return (x_train, y_train), (x_val, y_val), (xs, ys)\n",
        "\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error loading data: {e}\")\n",
        "\n",
        "# --------- 5. model factory -----------------------------------------\n",
        "class ThermalNN(tf.keras.Model):\n",
        "    \"\"\"Two-layer dense surrogate, same topology as original notebook.\"\"\"\n",
        "    def __init__(self, width=32, act=\"relu\", dropout_rate=0.0):\n",
        "        super().__init__()\n",
        "        self.g = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(width, activation=act),\n",
        "            tf.keras.layers.Dropout(dropout_rate),\n",
        "            tf.keras.layers.Dense(1)\n",
        "        ])\n",
        "        self.c = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(width, activation=act),\n",
        "            tf.keras.layers.Dropout(dropout_rate),\n",
        "            tf.keras.layers.Dense(1)\n",
        "        ])\n",
        "        self.p = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(width, activation=act),\n",
        "            tf.keras.layers.Dropout(dropout_rate),\n",
        "            tf.keras.layers.Dense(1)\n",
        "        ])\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        g = self.g(x, training=training)\n",
        "        c = self.c(x, training=training)\n",
        "        p = self.p(x, training=training)\n",
        "        return p / (g + 1e-7) + c\n",
        "\n",
        "def build_tnn(hp: kt.HyperParameters):\n",
        "    \"\"\"Build model with hyperparameters\"\"\"\n",
        "    width = hp.Int(\"width\", 16, 128, step=16)  # Adjusted range\n",
        "    act = hp.Choice(\"activation\", [\"relu\", \"elu\", \"swish\"])\n",
        "    lr = hp.Float(\"lr\", 1e-5, 1e-2, sampling=\"log\")\n",
        "    dropout = hp.Float(\"dropout\", 0.0, 0.3, step=0.1)\n",
        "\n",
        "    # Force model creation on GPU\n",
        "    with tf.device('/GPU:0'):\n",
        "        model = ThermalNN(width, act, dropout)\n",
        "\n",
        "        # Use mixed precision for better GPU performance\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=\"mse\",\n",
        "            metrics=[\"mse\", \"mae\"]\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# --------- 6. callbacks for better training -------------------------\n",
        "def get_callbacks(patience=6):\n",
        "    \"\"\"Get training callbacks\"\"\"\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=patience,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=3,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        ),\n",
        "        tf.keras.callbacks.LambdaCallback(\n",
        "            on_epoch_end=lambda epoch, logs: gc.collect() if epoch % 5 == 0 else None\n",
        "        )\n",
        "    ]\n",
        "    return callbacks\n",
        "\n",
        "# --------- 7. main training function --------------------------------\n",
        "def run_thermal_nn_tuning(config=None):\n",
        "    \"\"\"Main function to run hyperparameter tuning\"\"\"\n",
        "    if config is None:\n",
        "        config = CONFIG\n",
        "\n",
        "    print(\"🔥 Starting Thermal-NN Hyperparameter Tuning\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Setup GPU\n",
        "    gpu_available = setup_gpu()\n",
        "\n",
        "    # Load and prepare data\n",
        "    try:\n",
        "        (x_tr, y_tr), (x_val, y_val), scalers = load_scaled(\n",
        "            config['csv_path'],\n",
        "            config['val_split']\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Setup tuner\n",
        "    print(f\"\\n🔍 Starting hyperparameter search with {config['trials']} trials\")\n",
        "\n",
        "    tuner = kt.RandomSearch(\n",
        "        build_tnn,\n",
        "        objective=\"val_loss\",\n",
        "        max_trials=config['trials'],\n",
        "        directory=\"tuner_logs\",\n",
        "        project_name=\"thermal_nn\",\n",
        "        max_consecutive_failed_trials=max(5, config['trials'] // 4),\n",
        "        overwrite=True  # Clean start each run\n",
        "    )\n",
        "\n",
        "    # Memory cleanup callback\n",
        "    clear_cb = tf.keras.callbacks.LambdaCallback(\n",
        "        on_trial_end=lambda *_: (K.clear_session(), gc.collect(),\n",
        "                                 tf.keras.backend.clear_session())\n",
        "    )\n",
        "\n",
        "    # Start tuning\n",
        "    t0 = time.time()\n",
        "    try:\n",
        "        # Verify GPU usage before training\n",
        "        print(\"\\n🔍 Verifying GPU usage...\")\n",
        "\n",
        "        # Force data to GPU\n",
        "        with tf.device('/GPU:0'):\n",
        "            x_tr_gpu = tf.constant(x_tr)\n",
        "            y_tr_gpu = tf.constant(y_tr)\n",
        "            x_val_gpu = tf.constant(x_val)\n",
        "            y_val_gpu = tf.constant(y_val)\n",
        "            print(f\"✅ Training data moved to GPU: {x_tr_gpu.device}\")\n",
        "            print(f\"✅ Validation data moved to GPU: {x_val_gpu.device}\")\n",
        "\n",
        "        # Monitor GPU memory usage\n",
        "        def gpu_memory_callback():\n",
        "            if tf.config.list_physical_devices('GPU'):\n",
        "                gpu_info = tf.config.experimental.get_memory_info('GPU:0')\n",
        "                current_mb = gpu_info['current'] / 1024 / 1024\n",
        "                peak_mb = gpu_info['peak'] / 1024 / 1024\n",
        "                print(f\"🔥 GPU Memory - Current: {current_mb:.1f}MB, Peak: {peak_mb:.1f}MB\")\n",
        "\n",
        "        gpu_monitor_cb = tf.keras.callbacks.LambdaCallback(\n",
        "            on_epoch_begin=lambda epoch, logs: gpu_memory_callback() if epoch % 5 == 0 else None\n",
        "        )\n",
        "\n",
        "        tuner.search(\n",
        "            x_tr_gpu.numpy(), y_tr_gpu.numpy(),\n",
        "            epochs=config['epochs'],\n",
        "            batch_size=config['batch_size'],\n",
        "            validation_data=(x_val_gpu.numpy(), y_val_gpu.numpy()),\n",
        "            callbacks=[clear_cb, gpu_monitor_cb] + get_callbacks(patience=6),\n",
        "            verbose=1\n",
        "        )\n",
        "        print(f\"✅ Tuning completed in {time.time()-t0:.1f}s\")\n",
        "\n",
        "        # Get best model and retrain\n",
        "        best_hp = tuner.get_best_hyperparameters(1)[0]\n",
        "        print(f\"\\n🏆 Best hyperparameters:\")\n",
        "        for param, value in best_hp.values.items():\n",
        "            print(f\"  {param}: {value}\")\n",
        "\n",
        "        print(\"\\n🔄 Training final model with best hyperparameters...\")\n",
        "\n",
        "        # Force final model to GPU\n",
        "        with tf.device('/GPU:0'):\n",
        "            best_model = tuner.hypermodel.build(best_hp)\n",
        "\n",
        "        # Train final model with more epochs on GPU\n",
        "        with tf.device('/GPU:0'):\n",
        "            history = best_model.fit(\n",
        "                x_tr_gpu.numpy(), y_tr_gpu.numpy(),\n",
        "                validation_data=(x_val_gpu.numpy(), y_val_gpu.numpy()),\n",
        "                epochs=config['final_epochs'],\n",
        "                batch_size=config['batch_size'],\n",
        "                callbacks=[gpu_monitor_cb] + get_callbacks(patience=8),\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "        # Evaluate final model\n",
        "        with tf.device('/GPU:0'):\n",
        "            val_loss, val_mse, val_mae = best_model.evaluate(x_val_gpu.numpy(), y_val_gpu.numpy(), verbose=0)\n",
        "        print(f\"✅ Final validation metrics:\")\n",
        "        print(f\"  Loss: {val_loss:.4f}\")\n",
        "        print(f\"  MSE: {val_mse:.4f}\")\n",
        "        print(f\"  MAE: {val_mae:.4f}\")\n",
        "\n",
        "        # Final GPU memory check\n",
        "        if tf.config.list_physical_devices('GPU'):\n",
        "            gpu_info = tf.config.experimental.get_memory_info('GPU:0')\n",
        "            final_mb = gpu_info['current'] / 1024 / 1024\n",
        "            peak_mb = gpu_info['peak'] / 1024 / 1024\n",
        "            print(f\"🔥 Final GPU Memory - Current: {final_mb:.1f}MB, Peak: {peak_mb:.1f}MB\")\n",
        "\n",
        "        # Save results\n",
        "        art = Path(\"artifacts\")\n",
        "        art.mkdir(exist_ok=True)\n",
        "\n",
        "        best_model.save(art / \"tnn_best.keras\", include_optimizer=False)\n",
        "\n",
        "        metrics = {\n",
        "            \"val_loss\": float(val_loss),\n",
        "            \"val_mse\": float(val_mse),\n",
        "            \"val_mae\": float(val_mae),\n",
        "            \"hyperparameters\": best_hp.values,\n",
        "            \"training_time_seconds\": time.time() - t0,\n",
        "            \"gpu_used\": gpu_available,\n",
        "            \"config_used\": config\n",
        "        }\n",
        "\n",
        "        with open(art / \"metrics.json\", \"w\") as fp:\n",
        "            json.dump(metrics, fp, indent=2)\n",
        "\n",
        "        print(f\"\\n💾 Model saved to: {art / 'tnn_best.keras'}\")\n",
        "        print(f\"📊 Metrics saved to: {art / 'metrics.json'}\")\n",
        "\n",
        "        return best_model, metrics, history\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error during tuning: {e}\")\n",
        "        return None\n",
        "\n",
        "    finally:\n",
        "        # Final cleanup\n",
        "        K.clear_session()\n",
        "        gc.collect()\n",
        "\n",
        "# --------- 8. Helper functions for Colab ----------------------------\n",
        "def show_config():\n",
        "    \"\"\"Display current configuration\"\"\"\n",
        "    print(\"📋 Current Configuration:\")\n",
        "    print(\"-\" * 30)\n",
        "    for key, value in CONFIG.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "def update_config(**kwargs):\n",
        "    \"\"\"Update configuration parameters\"\"\"\n",
        "    for key, value in kwargs.items():\n",
        "        if key in CONFIG:\n",
        "            CONFIG[key] = value\n",
        "            print(f\"✅ Updated {key}: {value}\")\n",
        "        else:\n",
        "            print(f\"⚠️  Warning: {key} is not a valid config parameter\")\n",
        "\n",
        "# --------- 9. Ready to run! -----------------------------------------\n",
        "print(\"🚀 Thermal-NN Tuner loaded and ready!\")\n",
        "print(\"\\nTo run with default settings:\")\n",
        "print(\">>> result = run_thermal_nn_tuning()\")\n",
        "print(\"\\nTo see current config:\")\n",
        "print(\">>> show_config()\")\n",
        "print(\"\\nTo update config:\")\n",
        "print(\">>> update_config(trials=30, epochs=40)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HUEYiiqZwyF",
        "outputId": "6da41086-0386-4092-8055-641cc8bcd916"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Thermal-NN Tuner loaded and ready!\n",
            "\n",
            "To run with default settings:\n",
            ">>> result = run_thermal_nn_tuning()\n",
            "\n",
            "To see current config:\n",
            ">>> show_config()\n",
            "\n",
            "To update config:\n",
            ">>> update_config(trials=30, epochs=40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# CELL 2: GPU MONITORING SETUP - RUN THIS SECOND\n",
        "# ===============================================================\n",
        "\n",
        "# Force mixed precision for better GPU performance\n",
        "try:\n",
        "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "    tf.keras.mixed_precision.set_global_policy(policy)\n",
        "    print('✅ Mixed precision enabled (float16)')\n",
        "except:\n",
        "    print('⚠️  Mixed precision not available, using float32')\n",
        "\n",
        "# GPU monitoring functions\n",
        "def check_gpu_utilization():\n",
        "    \"\"\"Check if GPU is actually being used\"\"\"\n",
        "    import subprocess\n",
        "\n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "        try:\n",
        "            # Check nvidia-smi for GPU utilization\n",
        "            result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total',\n",
        "                                   '--format=csv,noheader,nounits'],\n",
        "                                  capture_output=True, text=True)\n",
        "            if result.returncode == 0:\n",
        "                gpu_util, mem_used, mem_total = result.stdout.strip().split(', ')\n",
        "                print(f\"🔥 GPU Utilization: {gpu_util}% | Memory: {mem_used}MB/{mem_total}MB\")\n",
        "                return int(gpu_util) > 0\n",
        "            else:\n",
        "                print(\"Could not get GPU utilization\")\n",
        "                return False\n",
        "        except:\n",
        "            print(\"nvidia-smi not available\")\n",
        "            return False\n",
        "    else:\n",
        "        print(\"No GPU available\")\n",
        "        return False\n",
        "\n",
        "def monitor_gpu_usage(duration=300):\n",
        "    \"\"\"Monitor GPU usage for specified duration (default 5 minutes)\"\"\"\n",
        "    import threading\n",
        "    import time\n",
        "\n",
        "    def monitor():\n",
        "        start_time = time.time()\n",
        "        while time.time() - start_time < duration:\n",
        "            check_gpu_utilization()\n",
        "            time.sleep(10)\n",
        "\n",
        "    thread = threading.Thread(target=monitor)\n",
        "    thread.daemon = True\n",
        "    thread.start()\n",
        "    return thread\n",
        "\n",
        "# Check current GPU status\n",
        "print(\"Current GPU status:\")\n",
        "check_gpu_utilization()\n",
        "\n",
        "print(\"\\n✅ GPU monitoring functions ready!\")\n",
        "print(\"Start monitoring with: monitor_thread = monitor_gpu_usage(300)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jULDeT_Ycayp",
        "outputId": "fb124543-e041-477a-d5ef-73ebd3b0028f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Mixed precision enabled (float16)\n",
            "Current GPU status:\n",
            "🔥 GPU Utilization: 0% | Memory: 278MB/15360MB\n",
            "\n",
            "✅ GPU monitoring functions ready!\n",
            "Start monitoring with: monitor_thread = monitor_gpu_usage(300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# CELL 3: CHECK AND MODIFY CONFIGURATION - RUN THIS THIRD\n",
        "# ===============================================================\n",
        "\n",
        "# Check current configuration\n",
        "show_config()\n",
        "\n",
        "# Optional: Modify configuration for better GPU utilization\n",
        "print(\"\\n🔧 Optimizing for GPU performance...\")\n",
        "update_config(\n",
        "    batch_size=1024,    # Larger batch for better GPU utilization\n",
        "    trials=15,          # Fewer trials but more focused\n",
        "    epochs=25           # Slightly fewer epochs per trial\n",
        ")\n",
        "\n",
        "print(\"\\n📁 Checking if data file exists...\")\n",
        "import os\n",
        "if os.path.exists(CONFIG['csv_path']):\n",
        "    print(f\"✅ Data file found: {CONFIG['csv_path']}\")\n",
        "else:\n",
        "    print(f\"❌ Data file not found: {CONFIG['csv_path']}\")\n",
        "    print(\"Please upload your CSV file or update the path in CONFIG\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GeSdWgicnky",
        "outputId": "ff1c9090-edc1-4cd0-c731-5beb257a1854"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📋 Current Configuration:\n",
            "------------------------------\n",
            "  csv_path: /content/measures_v2.csv\n",
            "  trials: 20\n",
            "  epochs: 30\n",
            "  final_epochs: 50\n",
            "  val_split: 0.2\n",
            "  batch_size: 512\n",
            "  random_seed: 42\n",
            "\n",
            "🔧 Optimizing for GPU performance...\n",
            "✅ Updated batch_size: 1024\n",
            "✅ Updated trials: 15\n",
            "✅ Updated epochs: 25\n",
            "\n",
            "📁 Checking if data file exists...\n",
            "✅ Data file found: /content/measures_v2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# CELL 4: START TRAINING - RUN THIS FOURTH\n",
        "# ===============================================================\n",
        "\n",
        "# Start GPU monitoring (optional but recommended)\n",
        "print(\"🔍 Starting GPU monitoring...\")\n",
        "monitor_thread = monitor_gpu_usage(600)  # Monitor for 10 minutes\n",
        "\n",
        "# Start training\n",
        "print(\"\\n🚀 Starting hyperparameter tuning...\")\n",
        "result = run_thermal_nn_tuning()\n",
        "\n",
        "if result:\n",
        "    best_model, metrics, history = result\n",
        "    print(\"\\n🎉 Training completed successfully!\")\n",
        "    print(\"Model and metrics saved in 'artifacts' folder\")\n",
        "else:\n",
        "    print(\"\\n❌ Training failed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YABWQw-ocxZf",
        "outputId": "a5c7d119-402e-47a4-fa56-92e3a066bb27"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 15 Complete [00h 00m 42s]\n",
            "val_loss: 2434.235595703125\n",
            "\n",
            "Best val_loss So Far: 348.73736572265625\n",
            "Total elapsed time: 00h 14m 38s\n",
            "✅ Tuning completed in 878.2s\n",
            "\n",
            "🏆 Best hyperparameters:\n",
            "  width: 48\n",
            "  activation: elu\n",
            "  lr: 0.0003204119265097899\n",
            "  dropout: 0.1\n",
            "\n",
            "🔄 Training final model with best hyperparameters...\n",
            "🔥 GPU Memory - Current: 171.7MB, Peak: 267.5MB\n",
            "Epoch 1/50\n",
            "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - loss: 75989.6094 - mae: 4.5771 - mse: 75989.6094 - val_loss: 3187.2876 - val_mae: 2.0370 - val_mse: 3187.2876 - learning_rate: 3.2041e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 371446080.0000 - mae: 30.5517 - mse: 371446080.0000 - val_loss: 159824.4375 - val_mae: 4.8457 - val_mse: 159824.4375 - learning_rate: 3.2041e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 52221.1992 - mae: 3.8650 - mse: 52221.1992 - val_loss: 18649.5664 - val_mae: 3.0233 - val_mse: 18649.5664 - learning_rate: 3.2041e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m1032/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2061196.2500 - mae: 7.5630 - mse: 2061196.2500\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00016020596376620233.\n",
            "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 2057123.0000 - mae: 7.5526 - mse: 2057123.0000 - val_loss: 12483.8525 - val_mae: 2.7043 - val_mse: 12483.8525 - learning_rate: 3.2041e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 60294.6133 - mae: 3.6200 - mse: 60294.6133 - val_loss: 6426.3901 - val_mae: 2.3571 - val_mse: 6426.3901 - learning_rate: 1.6021e-04\n",
            "🔥 GPU Memory - Current: 184.9MB, Peak: 267.5MB\n",
            "Epoch 6/50\n",
            "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 604938.3125 - mae: 4.0763 - mse: 604938.3125 - val_loss: 8663.4492 - val_mae: 2.1842 - val_mse: 8663.4492 - learning_rate: 1.6021e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m1034/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 41373.9609 - mae: 3.5333 - mse: 41373.9609\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 8.010298188310117e-05.\n",
            "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 41545.9727 - mae: 3.5344 - mse: 41545.9727 - val_loss: 4307.7739 - val_mae: 2.0305 - val_mse: 4307.7739 - learning_rate: 1.6021e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 4280431.0000 - mae: 6.3839 - mse: 4280431.0000 - val_loss: 3818.7183 - val_mae: 1.7948 - val_mse: 3818.7183 - learning_rate: 8.0103e-05\n",
            "Epoch 9/50\n",
            "\u001b[1m1040/1040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 551082.8125 - mae: 4.4776 - mse: 551082.8125 - val_loss: 69461.2500 - val_mae: 2.7044 - val_mse: 69461.2500 - learning_rate: 8.0103e-05\n",
            "Epoch 9: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "✅ Final validation metrics:\n",
            "  Loss: 3187.2996\n",
            "  MSE: 3187.2996\n",
            "  MAE: 2.0369\n",
            "🔥 Final GPU Memory - Current: 118.9MB, Peak: 267.5MB\n",
            "\n",
            "💾 Model saved to: artifacts/tnn_best.keras\n",
            "📊 Metrics saved to: artifacts/metrics.json\n",
            "\n",
            "🎉 Training completed successfully!\n",
            "Model and metrics saved in 'artifacts' folder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# CELL 5: CHECK RESULTS - RUN THIS AFTER TRAINING\n",
        "# ===============================================================\n",
        "\n",
        "# Check final GPU utilization\n",
        "print(\"Final GPU check:\")\n",
        "check_gpu_utilization()\n",
        "\n",
        "# Load and display results\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "artifacts_dir = Path(\"artifacts\")\n",
        "if artifacts_dir.exists():\n",
        "    # Display metrics\n",
        "    if (artifacts_dir / \"metrics.json\").exists():\n",
        "        with open(artifacts_dir / \"metrics.json\", \"r\") as f:\n",
        "            metrics = json.load(f)\n",
        "\n",
        "        print(\"\\n📊 Final Results:\")\n",
        "        print(\"=\" * 40)\n",
        "        print(f\"Validation Loss: {metrics['val_loss']:.4f}\")\n",
        "        print(f\"Validation MSE: {metrics['val_mse']:.4f}\")\n",
        "        print(f\"Validation MAE: {metrics['val_mae']:.4f}\")\n",
        "        print(f\"Training Time: {metrics['training_time_seconds']:.1f} seconds\")\n",
        "        print(f\"GPU Used: {metrics['gpu_used']}\")\n",
        "\n",
        "        print(f\"\\n🏆 Best Hyperparameters:\")\n",
        "        for param, value in metrics['hyperparameters'].items():\n",
        "            print(f\"  {param}: {value}\")\n",
        "\n",
        "    # List saved files\n",
        "    print(f\"\\n💾 Saved Files:\")\n",
        "    for file in artifacts_dir.iterdir():\n",
        "        print(f\"  {file.name} ({file.stat().st_size / 1024:.1f} KB)\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ No artifacts directory found - training may have failed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cILOROBfc17D",
        "outputId": "3fc35a0d-401c-47f4-dfc3-0f8b7c006a24"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final GPU check:\n",
            "🔥 GPU Utilization: 0% | Memory: 410MB/15360MB\n",
            "\n",
            "📊 Final Results:\n",
            "========================================\n",
            "Validation Loss: 3187.2996\n",
            "Validation MSE: 3187.2996\n",
            "Validation MAE: 2.0369\n",
            "Training Time: 944.6 seconds\n",
            "GPU Used: True\n",
            "\n",
            "🏆 Best Hyperparameters:\n",
            "  width: 48\n",
            "  activation: elu\n",
            "  lr: 0.0003204119265097899\n",
            "  dropout: 0.1\n",
            "\n",
            "💾 Saved Files:\n",
            "  metrics.json (0.5 KB)\n",
            "  tnn_best.keras (72.2 KB)\n"
          ]
        }
      ]
    }
  ]
}